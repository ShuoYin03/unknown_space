---
title: "Natural Language Processing - LLM"
subtitle: "BERT & GPT, how did they trained?"
description: "Introduce to the pre-train and tuning of BERT and GPT large language model"
date: 2024-01-18
author: "UNKNOWN SPACE"
image: ""
tags: ["nlp"]
categories: ["notes"]
URL: "/2024/01/18/nlp-llm/"
math: true
draft: true
---

## LLM Architecture
An LLM contains large amount of parameters like input vectors or tokens

## BERT Training
`BERT`, stands for **Bidirectional Encoder Representations from Transformers**, uses `transformer encoder` structure as the name describes. It takes either a single sentence or a sentence pair as an input. 
- A sentence input is a **sequence of tokens**, starting with [CLS]  
"[CLS] my dog is cute."
- A sentence pair is a **combined sequence of tokens from two sentences**, starting with [CLS] and inserting [SEP] in between two sentences.  
"[CLS] my dog is cute [SEP] he likes playing."

### BERT Pre-training
BERT is pre-trained on two learning tasks: `MLM` (masked language model)  pre-training & `NSP` (next sentence prediction) pre-training.

#### MLM
Use 15% tokens, with each token, 10 percent keep the same, 80 percent change a word to mask, another 10 percent alternate to another word, and based on these things, predict the mask part
#### NSP

### BERT Fine-tuning
Based on 9 English sentence understanding tasks, everyone of them is translated to a classification task. 2 of them is 
1. Single sentence classification (2 tasks)  
- `CoLA`: To predict whether an English sentence is linguistically “acceptable” or not.  
- `SST2`: To predict sentiment (positive or negative) of movie reviews.  
2. Sentence pair classification (3 tasks)  
- `MRPC`: To predict whether the sentences in the pair are semantically equivalent.  
- `QQP`: To predict if two questions asked on Quora are semantically equivalent.  
- `STS-B`: To predict how semantically similar two sentences are with a score from 1 to 5.  
3. Natural language inference (4 tasks)  
- `MNLI`, RTE (smaller dataset): To predict, given a sentence pair, whether the 2nd sentence is an entailment, contradiction, or neutral with respect to the 1st one.  
- `SQuAD`: Given a question and a passage from Wikipedia containing the answer, to predict the answer text span in the passage.  
- `Winograd Schema Challenge`

## GPT Training
`GPT`, stands for **Generative Pre-trained Transformers**, uses a transformer decoder structure

### GPT Pre-training

$$\max \sum_i \log p(w_i | w_{i-1}, w_{i-2}, \ldots, w_{i-k} \text{ input context})$$
### GPT Fine-tuning

## Advanced LLM Training Techniques

### Instruction fine-tuning
1. Without exemplar
2. With exemplars
### Chain of thought fine-tuning
Goal: Improve LLM performance on unseen reasoning tasks, 
### Learning from human feedback

## Recent LLM Trend
